{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "6864Proj_2recursivelstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obLpwjWQ3mD7"
      },
      "source": [
        "# Get Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drdsVVhwL2pb",
        "outputId": "eab33ff2-699c-476d-cd6b-f3e7e792425e"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "data_dir = \"./parsed_chat_data.csv\"\n",
        "\n",
        "if not os.path.exists(data_dir):\n",
        "  # !wget https://raw.githubusercontent.com/stran123/6.864-final-project/main/data/mini_parsed_data.csv -O ./mini_data.csv\n",
        "  !wget https://raw.githubusercontent.com/stran123/6.864-final-project/main/data/parsed_chat_data.zip -O ./parsed_chat_data.zip\n",
        "  !unzip parsed_chat_data.zip -d ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "--2021-05-20 05:33:06--  https://raw.githubusercontent.com/stran123/6.864-final-project/main/data/parsed_chat_data.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10794217 (10M) [application/zip]\n",
            "Saving to: ‘./parsed_chat_data.zip’\n",
            "\n",
            "./parsed_chat_data. 100%[===================>]  10.29M  48.9MB/s    in 0.2s    \n",
            "\n",
            "2021-05-20 05:33:07 (48.9 MB/s) - ‘./parsed_chat_data.zip’ saved [10794217/10794217]\n",
            "\n",
            "Archive:  parsed_chat_data.zip\n",
            "  inflating: ./parsed_chat_data.csv  \n",
            "  inflating: ./__MACOSX/._parsed_chat_data.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPDFr8iih9EU"
      },
      "source": [
        "# Preprocessing (parse messages)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em5NtB_TiA8F"
      },
      "source": [
        "# import spacy\n",
        "# tokenizer = spacy.load('en_core_web_sm')\n",
        "\n",
        "# tokenized_msgs = []\n",
        "\n",
        "# df['parsed_msg'] = None\n",
        "# for i, msg in tqdm(enumerate(df['message'].values[:100])):\n",
        "#   tokenized_msg = tokenizer(msg[1:])\n",
        "#   df['parsed_msg'][i]=' '.join([token.text for token in tokenized_msg])\n",
        "\n",
        "# df.to_csv('mini_parsed_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIYu4v0i3qjX"
      },
      "source": [
        "# Read Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nzq0URsP5fxp"
      },
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "df = pd.read_csv(data_dir)\n",
        "df = df[['conversation_id','message','parsed_msg']][:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQjQmuivv9EA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "590464ef-9d2d-4d7f-eb2e-8c9109e12b7c"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>message</th>\n",
              "      <th>parsed_msg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Are you a fan of Google or Microsoft?</td>\n",
              "      <td>Are you a fan of Google or Microsoft ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Both are excellent technology they are helpfu...</td>\n",
              "      <td>Both are excellent technology they are helpful...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>I'm not  a huge fan of Google, but I use it a...</td>\n",
              "      <td>I 'm not   a huge fan of Google , but I use it...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Google provides online related services and p...</td>\n",
              "      <td>Google provides online related services and pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>Yeah, their services are good. I'm just not a...</td>\n",
              "      <td>Yeah , their services are good . I 'm just not...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188373</th>\n",
              "      <td>8628</td>\n",
              "      <td>Wow, it does not seem like that long. Since I...</td>\n",
              "      <td>Wow , it does not seem like that long . Since ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188374</th>\n",
              "      <td>8628</td>\n",
              "      <td>I havent seen that episode, I might google it...</td>\n",
              "      <td>I have nt seen that episode , I might google i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188375</th>\n",
              "      <td>8628</td>\n",
              "      <td>I don't think I have either. That's an insane...</td>\n",
              "      <td>I do n't think I have either . That 's an insa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188376</th>\n",
              "      <td>8628</td>\n",
              "      <td>I did, my little brother used to love Thomas ...</td>\n",
              "      <td>I did , my little brother used to love Thomas ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188377</th>\n",
              "      <td>8628</td>\n",
              "      <td>It did. Ringo Starr, George Carlin, and Alec ...</td>\n",
              "      <td>It did . Ringo Starr , George Carlin , and Ale...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>188378 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        conversation_id  ...                                         parsed_msg\n",
              "0                     1  ...             Are you a fan of Google or Microsoft ?\n",
              "1                     1  ...  Both are excellent technology they are helpful...\n",
              "2                     1  ...  I 'm not   a huge fan of Google , but I use it...\n",
              "3                     1  ...  Google provides online related services and pr...\n",
              "4                     1  ...  Yeah , their services are good . I 'm just not...\n",
              "...                 ...  ...                                                ...\n",
              "188373             8628  ...  Wow , it does not seem like that long . Since ...\n",
              "188374             8628  ...  I have nt seen that episode , I might google i...\n",
              "188375             8628  ...  I do n't think I have either . That 's an insa...\n",
              "188376             8628  ...  I did , my little brother used to love Thomas ...\n",
              "188377             8628  ...  It did . Ringo Starr , George Carlin , and Ale...\n",
              "\n",
              "[188378 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1Quz4lE36lx"
      },
      "source": [
        "# Add START and PAD tokens. Split data into train/val"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P61QJXHlvF8v"
      },
      "source": [
        "START=['[start]']\n",
        "PAD=['[pad]']\n",
        "END = ['[end]']\n",
        "\n",
        "longest_msg = START + max(df['parsed_msg'], key=lambda x: len(str(x).strip().split(' '))).split(' ') + END\n",
        "num_convos = max(df['conversation_id'], key=lambda x: int(x))\n",
        "\n",
        "train_data = []\n",
        "valid_data = []\n",
        "vocab = {START[0], PAD[0], END[0]}\n",
        "prev_conv_id = 0\n",
        "training_over = False\n",
        "training_split = 0.8\n",
        "\n",
        "for conv_id, _, parsed_msg in df.values:\n",
        "  if conv_id > num_convos*0.8:\n",
        "    training_over = True\n",
        "\n",
        "  if prev_conv_id != conv_id:\n",
        "    if not training_over:\n",
        "      train_data.append(START+END+PAD*(len(longest_msg)-2))\n",
        "    else:\n",
        "      valid_data.append(START+END+PAD*(len(longest_msg)-2))\n",
        "    prev_conv_id = conv_id\n",
        "\n",
        "  parsed_msg_split = str(parsed_msg).strip().lower().split(' ')\n",
        "  vocab |= set(parsed_msg_split)\n",
        "\n",
        "  if not training_over:\n",
        "    train_data.append(START + parsed_msg_split + END +PAD*(len(longest_msg)-len(parsed_msg_split)-2))\n",
        "  else:\n",
        "    valid_data.append(START + parsed_msg_split + END +PAD*(len(longest_msg)-len(parsed_msg_split)-2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7V8VMWYs5lIn"
      },
      "source": [
        "import pickle\n",
        "load = True\n",
        "if load:\n",
        "  with open(\"vocab_list_NewDecoder.pkl\", \"rb\") as f:\n",
        "    vocab = pickle.load(f) \n",
        "else:\n",
        "  vocab = list(vocab)\n",
        "vocab_oh = {w:i for i,w in enumerate(vocab)}\n",
        "train_data_encoded = []\n",
        "valid_data_encoded = []\n",
        "PAD_INDEX = vocab_oh[\"[pad]\"]\n",
        "for msg in train_data:\n",
        "  msg_vectorized = []\n",
        "  for word in msg:\n",
        "    msg_vectorized.append(vocab_oh[word.lower()])\n",
        "  train_data_encoded.append(msg_vectorized)\n",
        "\n",
        "for msg in valid_data:\n",
        "  msg_vectorized = []\n",
        "  for word in msg:\n",
        "    msg_vectorized.append(vocab_oh[word.lower()])\n",
        "  valid_data_encoded.append(msg_vectorized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6A7hS_LFJFzO",
        "outputId": "c896ccfa-66ea-4114-d580-8de6004ac308"
      },
      "source": [
        "print(train_data[1])\n",
        "print(train_data[2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[start]', 'are', 'you', 'a', 'fan', 'of', 'google', 'or', 'microsoft', '?', '[end]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]']\n",
            "['[start]', 'both', 'are', 'excellent', 'technology', 'they', 'are', 'helpful', 'in', 'many', 'ways', '.', 'for', 'the', 'security', 'purpose', 'both', 'are', 'super', '.', '[end]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAeDpKaFeDHL"
      },
      "source": [
        "def get_len(sent):\n",
        "  i = 0\n",
        "  for i, elt in enumerate(sent):\n",
        "    if elt == vocab_oh[\"[pad]\"]:\n",
        "      return i\n",
        "  return i + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enoZ_ji5k_0l"
      },
      "source": [
        "from torch.utils import data\n",
        "class ChatBotDataset(data.Dataset):\n",
        "  def __init__(self, train_data):\n",
        "    self.data = train_data\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  def __getitem__(self,index):\n",
        "    self.src, self.src_len, self.tgt, self.tgt_len = self.data[index]\n",
        "    return self.src, self.src_len, self.tgt, self.tgt_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vSN-AogeDaR",
        "outputId": "df32eb3f-8b8a-44b1-92e2-227acccc3007"
      },
      "source": [
        "import random\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "random.seed(0)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "training_data_x = torch.tensor(train_data_encoded)\n",
        "validation_data_x = torch.tensor(valid_data_encoded)\n",
        "training_data_y = torch.cat((training_data_x[1:,:], training_data_x[0:1,:]), axis=0)\n",
        "validation_data_y = torch.cat((validation_data_x[1:,:], validation_data_x[0:1,:]), axis=0)\n",
        "\n",
        "training_data = list(zip(training_data_x, training_data_y))\n",
        "validation_data = list(zip(validation_data_x, validation_data_y))\n",
        "\n",
        "all_data = training_data + validation_data\n",
        "random.shuffle(all_data)\n",
        "\n",
        "training_data = all_data[:int(len(all_data)*training_split)]\n",
        "validation_data = all_data[int(len(all_data)*training_split):]\n",
        "\n",
        "# training_data  = training_data[:]\n",
        "# validation_data = validation_data[:]\n",
        "\n",
        "\n",
        "train_src, train_tgt = zip(*training_data)\n",
        "train_src, train_tgt = torch.cat(train_src,dim=0).reshape(-1,len(longest_msg)), torch.cat(train_tgt,dim=0).reshape(-1,len(longest_msg))\n",
        "train_dataset = ChatBotDataset(list(zip(train_src, map(get_len, train_src), train_tgt, map(get_len,train_tgt))))\n",
        "train_data_loader = data.DataLoader(train_dataset, batch_size=batch_size, num_workers=4, shuffle=True)\n",
        "\n",
        "val_src, val_tgt = zip(*validation_data)\n",
        "val_src, val_tgt = torch.cat(val_src,dim=0).reshape(-1,len(longest_msg)), torch.cat(val_tgt,dim=0).reshape(-1,len(longest_msg))\n",
        "\n",
        "val_dataset = ChatBotDataset(list(zip(val_src, map(get_len, val_src), val_tgt, map(get_len,val_tgt))))\n",
        "val_data_loader = data.DataLoader(val_dataset, batch_size=batch_size, num_workers=4, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koJNAOdO30Kl"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kC0GRQfbW0F"
      },
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "# class Embedder(nn.Module):\n",
        "#   def __init__(self, num_words, num_layers, hidden_size, dropout=0):\n",
        "#     super(Embedder, self).__init__()\n",
        "\n",
        "#     self.embedding = nn.Embedding(num_words, hidden_size)\n",
        "\n",
        "#     self.lstm = nn.LSTM(input_size=hidden_size,\n",
        "#                         hidden_size=hidden_size,\n",
        "#                         num_layers=num_layers,\n",
        "#                         batch_first=True,\n",
        "#                         dropout=dropout,\n",
        "#                         bidirectional=True)\n",
        "  \n",
        "#   def forward(self, x):\n",
        "#     x = self.embedding(x)\n",
        "#     outputs, hidden = self.lstm(x)\n",
        "#     return outputs\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, dropout=0.):\n",
        "    \"\"\"\n",
        "    Inputs: \n",
        "      - `input_size`: an int representing the RNN input size.\n",
        "      - `hidden_size`: an int representing the RNN hidden size.\n",
        "      - `dropout`: a float representing the dropout rate during training. Note\n",
        "          that for 1-layer RNN this has no effect since dropout only applies to\n",
        "          outputs of intermediate layers.\n",
        "    \"\"\"\n",
        "    \n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    # --------- Your code here --------- #\n",
        "    # feel free to use a pre-implemented pytorch GRU\n",
        "    # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "    self.rnn = nn.GRU(input_size, hidden_size, num_layers=1, batch_first=True, bidirectional=True, dropout=dropout) \n",
        "    \n",
        "    # --------- Your code ends --------- #\n",
        "\n",
        "  def forward(self, inputs, lengths):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n",
        "          representing a batch of padded embedded word vectors of source\n",
        "          sentences.\n",
        "      - `lengths`: a 1d-tensor of shape (batch_size,) representing the sequence\n",
        "          lengths of `inputs`.\n",
        "\n",
        "    Returns:\n",
        "      - `outputs`: a 3d-tensor of shape\n",
        "        (batch_size, max_seq_length, hidden_size).\n",
        "      - `finals`: a 3d-tensor of shape (num_layers, batch_size, hidden_size).\n",
        "\n",
        "      Hint: `outputs` and `finals` are both standard GRU outputs.\n",
        "    \"\"\"\n",
        "    outputs = None\n",
        "    finals = None\n",
        "    # --------- Your code here --------- #\n",
        "    # hint: you probably want to pack the inputs and outputs (see note below)\n",
        "    #       https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html\n",
        "    # hint2: given the shape of the inputs and outputs, \n",
        "    #        it might be helpful to specify batch_first=True (also in __init___)\n",
        "    # hint3: MAX_SENT_LENGTH_PLUS_SOS_EOS is a global variable that exists if \n",
        "    #        you ever need to specify a total_length for outputs\n",
        "    packed = pack_padded_sequence(inputs, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "    outputs, finals = self.rnn(packed)\n",
        "    outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n",
        "\n",
        "    # we need to manually concatenate the final states for both directions\n",
        "    fwd_final = finals[0:finals.size(0):2]\n",
        "    bwd_final = finals[1:finals.size(0):2]\n",
        "    finals = torch.cat([fwd_final, bwd_final], dim=2)  # [num_layers, batch, 2*dim]\n",
        "\n",
        "    # --------- Your code ends --------- #\n",
        "\n",
        "    return outputs, finals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01iljwCzRn5h"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  \"\"\"An RNN decoder without attention.\"\"\"\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, dropout=0.):\n",
        "    \"\"\"\n",
        "      Inputs:\n",
        "        - `input_size`, `hidden_size`, and `dropout` the same as in Encoder.\n",
        "    \"\"\"\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    # --------- Your code here --------- #\n",
        "    # hint: you need more layers than the encoder\n",
        "    #       again, feel free to use pytorch implemetnations\n",
        "    \n",
        "    self.rnn = nn.GRU(input_size, hidden_size, batch_first=True,\n",
        "                      dropout=dropout)\n",
        "\n",
        "    # To initialize from the final encoder state.\n",
        "    self.bridge = nn.Linear(2*hidden_size, hidden_size, bias=True)\n",
        "\n",
        "    self.dropout_layer = nn.Dropout(p=dropout)\n",
        "    self.pre_output_layer = nn.Linear(hidden_size + input_size, hidden_size,\n",
        "                                      bias=False)\n",
        "    \n",
        "    # --------- Your code ends --------- #\n",
        "\n",
        "  def forward_step(self, prev_embed, hidden):\n",
        "    \"\"\"Helper function for forward below:\n",
        "       Perform a single decoder step (1 word).\n",
        "\n",
        "       Inputs:\n",
        "      - `prev_embed`: a 3d-tensor of shape (batch_size, 1, embed_size)\n",
        "          representing the padded embedded word vectors at this step in training\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n",
        "          the current hidden state.\n",
        "\n",
        "      Returns:\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n",
        "          representing the current decoder hidden state.\n",
        "      - `pre_output`: a 3d-tensor of shape (batch_size, 1, hidden_size)\n",
        "          representing the total decoder output for one step\n",
        "    \"\"\"\n",
        "    pre_output = None\n",
        "    # --------- Your code here --------- #\n",
        "    pre_output, hidden = self.rnn(prev_embed, hidden) # minimun necessary\n",
        "\n",
        "    # this is \"extra\"\n",
        "    pre_output = torch.cat([prev_embed, pre_output], dim=2)\n",
        "\n",
        "    pre_output = self.dropout_layer(pre_output)\n",
        "    pre_output = self.pre_output_layer(pre_output)\n",
        "    # --------- Your code ends --------- #\n",
        "    return hidden, pre_output\n",
        "\n",
        "  def forward(self, inputs, encoder_finals, hidden=None, max_len=None):\n",
        "    \"\"\"Unroll the decoder one step at a time.\n",
        "\n",
        "    Inputs:\n",
        "      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n",
        "          representing a batch of padded embedded word vectors of target\n",
        "          sentences (for teacher-forcing during training).\n",
        "      - `encoder_finals`: a 3d-tensor of shape\n",
        "          (num_enc_layers, batch_size, hidden_size) representing the final\n",
        "          encoder hidden states used to initialize the initial decoder hidden\n",
        "          states.\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n",
        "          the value to be used to initialize the initial decoder hidden states.\n",
        "          If None, then use `encoder_finals`.\n",
        "      - `max_len`: an int representing the maximum decoding length.\n",
        "\n",
        "    Returns:\n",
        "      - `outputs`: a 3d-tensor of shape\n",
        "          (batch_size, max_seq_length, hidden_size) representing the raw\n",
        "          decoder outputs (before converting to a `trg_vocab_size`-dim vector).\n",
        "          We will convert it later in a `Generator` below.\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n",
        "          representing the last decoder hidden state.\n",
        "    \"\"\"\n",
        "\n",
        "    # The maximum number of steps to unroll the RNN.\n",
        "    if max_len is None:\n",
        "      max_len = inputs.size(1)\n",
        "\n",
        "    # Initialize decoder hidden state.\n",
        "    if hidden is None:\n",
        "      hidden = self.init_hidden(encoder_finals)\n",
        "\n",
        "    outputs = None\n",
        "    \n",
        "    # --------- Your code here --------- #\n",
        "\n",
        "    # Unroll the decoder RNN for `max_len` steps.\n",
        "    # hint: use the above helper function forward_step that \n",
        "    #       performs a single decoder step (1 word).\n",
        "\n",
        "    # Here we store all intermediate hidden states and pre-output vectors.\n",
        "    pre_output_vectors = []\n",
        "\n",
        "    # Unroll the decoder RNN for `max_len` steps.\n",
        "    for i in range(max_len):\n",
        "      prev_embed = inputs[:, i].unsqueeze(1)\n",
        "      hidden, pre_output = self.forward_step(prev_embed, hidden)\n",
        "      pre_output_vectors.append(pre_output)\n",
        "\n",
        "    outputs = torch.cat(pre_output_vectors, dim=1)\n",
        "  \n",
        "    # --------- Your code ends --------- #\n",
        "    return hidden, outputs\n",
        "\n",
        "  def init_hidden(self, encoder_finals):\n",
        "    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n",
        "    state.\"\"\"\n",
        "    decoder_init_hiddens = torch.tanh(self.bridge(encoder_finals))\n",
        "    \n",
        "    return decoder_init_hiddens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCCserB11q2k"
      },
      "source": [
        "# class Predictor(nn.Module):\n",
        "    \n",
        "#   def __init__(self, num_words, num_layers, hidden_size, dropout=0):\n",
        "#     super(Predictor, self).__init__()\n",
        "#     self.embedder = Embedder(num_words, num_layers, hidden_size)\n",
        "#     self.hidden_size = hidden_size\n",
        "#     self.dropout = nn.Dropout(dropout)\n",
        "#     self.lstm = nn.LSTM(input_size=2*hidden_size,\n",
        "#                         hidden_size=hidden_size,\n",
        "#                         num_layers=num_layers,\n",
        "#                         batch_first=True,\n",
        "#                         dropout=dropout,\n",
        "#                         bidirectional=True)\n",
        "#     self.proj = nn.Linear(2*hidden_size, num_words, bias=True)\n",
        "#     self.logsoftmax = nn.LogSoftmax(dim=2)\n",
        "  \n",
        "#   def forward(self, x):\n",
        "#     word_embeddings = self.embedder(x)\n",
        "#     word_embeddings = self.dropout(word_embeddings)\n",
        "#     output, (_, _) = self.lstm(word_embeddings)\n",
        "#     decoded_output = self.proj(output) # [batch_size, max_len, vocab_size]\n",
        "#     # return decoded_output\n",
        "#     return self.logsoftmax(decoded_output) # [batch_size, max_len, vocab_size]\n",
        "\n",
        "\n",
        "class EncoderDecoder(nn.Module):\n",
        "  \"\"\"A standard Encoder-Decoder architecture without attention.\n",
        "  \"\"\"\n",
        "  def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      - `encoder`: an `Encoder` object.\n",
        "      - `decoder`: a `Decoder` object.\n",
        "      - `src_embed`: an nn.Embedding object representing the lookup table for\n",
        "          input (source) sentences.\n",
        "      - `trg_embed`: an nn.Embedding object representing the lookup table for\n",
        "          output (target) sentences.\n",
        "      - `generator`: a `Generator` object. Essentially a linear mapping. See\n",
        "          the next code cell.\n",
        "    \"\"\"\n",
        "    super(EncoderDecoder, self).__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.trg_embed = trg_embed\n",
        "    self.generator = generator\n",
        "\n",
        "  def forward(self, src_ids, trg_ids, src_lengths):\n",
        "    \"\"\"Take in and process masked source and target sequences.\n",
        "\n",
        "    Inputs:\n",
        "      `src_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n",
        "        a batch of source sentences of word ids.\n",
        "      `trg_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n",
        "        a batch of target sentences of word ids.\n",
        "      `src_lengths`: a 1d-tensor of shape (batch_size,) representing the\n",
        "        sequence length of `src_ids`.\n",
        "\n",
        "    Returns the decoder outputs, see the above cell.\n",
        "    \"\"\"\n",
        "    encoder_hiddens, encoder_finals = self.encode(src_ids, src_lengths)\n",
        "    del encoder_hiddens   # unused\n",
        "    return self.decode(encoder_finals, trg_ids[:, :-1])\n",
        "\n",
        "  def encode(self, src_ids, src_lengths):\n",
        "    return self.encoder(self.src_embed(src_ids), src_lengths)\n",
        "    \n",
        "  def decode(self, encoder_finals, trg_ids, decoder_hidden=None):\n",
        "    return self.decoder(self.trg_embed(trg_ids), encoder_finals, decoder_hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc3YZvr3UQaN"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  \"\"\"Define standard linear + softmax generation step.\"\"\"\n",
        "  def __init__(self, hidden_size, vocab_size):\n",
        "    super(Generator, self).__init__()\n",
        "    self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy6wn8apU9Wk"
      },
      "source": [
        "import math\n",
        "\n",
        "\n",
        "class SimpleLossCompute:\n",
        "  \"\"\"A simple loss compute and train function.\"\"\"\n",
        "\n",
        "  def __init__(self, generator, criterion, opt=None):\n",
        "    self.generator = generator\n",
        "    self.criterion = criterion\n",
        "    self.opt = opt\n",
        "\n",
        "  def __call__(self, x, y, norm):\n",
        "    x = self.generator(x)\n",
        "    loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
        "                          y.contiguous().view(-1))\n",
        "    loss = loss / norm\n",
        "\n",
        "    if self.opt is not None:  # training mode\n",
        "      loss.backward()          \n",
        "      self.opt.step()\n",
        "      self.opt.zero_grad()\n",
        "\n",
        "    return loss.data.item() * norm\n",
        "\n",
        "\n",
        "def run_epoch(data_loader, model, loss_compute, print_every):\n",
        "  \"\"\"Standard Training and Logging Function\"\"\"\n",
        "\n",
        "  total_tokens = 0\n",
        "  total_loss = 0\n",
        "\n",
        "  for i, (src_ids_BxT, src_lengths_B, trg_ids_BxL, trg_lengths_B) in enumerate(tqdm(data_loader)):\n",
        "    # We define some notations here to help you understand the loaded tensor\n",
        "    # shapes:\n",
        "    #   `B`: batch size\n",
        "    #   `T`: max sequence length of source sentences\n",
        "    #   `L`: max sequence length of target sentences; due to our preprocessing\n",
        "    #        in the beginning, `L` == `T` == 50\n",
        "    # An example of `src_ids_BxT` (when B = 2):\n",
        "    #   [[2, 4, 6, 7, ..., 4, 3, 0, 0, 0],\n",
        "    #    [2, 8, 6, 5, ..., 9, 5, 4, 3, 0]]\n",
        "    # The corresponding `src_lengths_B` would be [47, 49].\n",
        "    # Note that SOS_INDEX == 2, EOS_INDEX == 3, and PAD_INDEX = 0.\n",
        "\n",
        "    src_ids_BxT = src_ids_BxT.to(device)\n",
        "    src_lengths_B = src_lengths_B.to(device)\n",
        "    trg_ids_BxL = trg_ids_BxL.to(device)\n",
        "\n",
        "    del trg_lengths_B   # unused\n",
        "\n",
        "    _, output = model(src_ids_BxT, trg_ids_BxL, src_lengths_B)\n",
        "\n",
        "    loss = loss_compute(x=output, y=trg_ids_BxL[:, 1:],\n",
        "                        norm=src_ids_BxT.size(0))\n",
        "    total_loss += loss\n",
        "    total_tokens += (trg_ids_BxL[:, 1:] != PAD_INDEX).data.sum().item()\n",
        "\n",
        "    if model.training and i % print_every == 0:\n",
        "      print(\"Epoch Step: %d Loss: %f\" % (i, loss / src_ids_BxT.size(0)))\n",
        "\n",
        "  return math.exp(total_loss / float(total_tokens))\n",
        "\n",
        "\n",
        "def train(model, num_epochs, learning_rate, print_every):\n",
        "  # Set `ignore_index` as PAD_INDEX so that pad tokens won't be included when\n",
        "  # computing the loss.\n",
        "  criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=PAD_INDEX)\n",
        "  optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  # Keep track of dev ppl for each epoch.\n",
        "  dev_ppls = []\n",
        "\n",
        "  for epoch in tqdm(range(num_epochs)):\n",
        "    print(\"Epoch\", epoch)\n",
        "\n",
        "    model.train()\n",
        "    train_ppl = run_epoch(data_loader=train_data_loader, model=model,\n",
        "                          loss_compute=SimpleLossCompute(model.generator,\n",
        "                                                         criterion, optim),\n",
        "                          print_every=print_every)\n",
        "        \n",
        "    model.eval()\n",
        "    with torch.no_grad():      \n",
        "      dev_ppl = run_epoch(data_loader=val_data_loader, model=model,\n",
        "                          loss_compute=SimpleLossCompute(model.generator,\n",
        "                                                         criterion, None),\n",
        "                          print_every=print_every)\n",
        "      print(\"Validation perplexity: %f\" % dev_ppl)\n",
        "      dev_ppls.append(dev_ppl)\n",
        "        \n",
        "  return dev_ppls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSnw6GN7riZB"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfDyIshKrkp1"
      },
      "source": [
        "\n",
        "# learning_rate = 1e-3\n",
        "# dropout = 0.5\n",
        "# epochs = 30\n",
        "# loss_fn = nn.NLLLoss(reduction=\"sum\", ignore_index=vocab_oh['[pad]'])\n",
        "# hidden_size = 1024\n",
        "# embedder_layers = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehtb5s2Yrbgs"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av1f9WmFtPRM"
      },
      "source": [
        "## Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b9EAX67-KpY"
      },
      "source": [
        "#model = Predictor(len(vocab), embedder_layers, hidden_size, dropout)\n",
        "#train_losses = []\n",
        "#test_losses = []\n",
        "#print_every = 30\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "MODEL_FOLDER = \"/content/drive/MyDrive/mit-6864/final-project\"\n",
        "MODEL_NAME = \"original_model\"\n",
        "!mkdir -p \"/content/drive/MyDrive/mit-6864/final-project\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC6XEBCtfGyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d413d9a-73d2-4b6d-d0f3-31d5fe119e76"
      },
      "source": [
        "import pickle\n",
        "# Hyperparameters for contructing the encoder-decoder model.\n",
        "embed_size = 256   # Each word will be represented as a `embed_size`-dim vector.\n",
        "hidden_size = 256  # RNN hidden size.\n",
        "dropout = 0.2\n",
        "\n",
        "model = EncoderDecoder(\n",
        "    encoder=Encoder(embed_size, hidden_size, dropout=dropout),\n",
        "    decoder=Decoder(embed_size, hidden_size, dropout=dropout),\n",
        "    src_embed=nn.Embedding(len(vocab), embed_size),\n",
        "    trg_embed=nn.Embedding(len(vocab), embed_size),\n",
        "    generator=Generator(hidden_size, len(vocab))).to(device)\n",
        "\n",
        "train_model = False\n",
        "if train_model:\n",
        "  # Start training. The returned `dev_ppls` is a list of dev perplexity for each\n",
        "  # epoch.\n",
        "  model_ppls = train(model, num_epochs=10, learning_rate=1e-3,\n",
        "                        print_every=100)\n",
        "  \n",
        "  torch.save(model.state_dict(), MODEL_FOLDER+\"/\" + \"modelNewDecoder.pt\")\n",
        "  with open(\"vocab_list_NewDecoder.pkl\", \"wb\") as f:\n",
        "    pickle.dump(vocab,f)\n",
        "\n",
        "else:\n",
        "  model.load_state_dict(torch.load(\"modelNewDecoderV3.pt\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1AG5rlpu2fh"
      },
      "source": [
        "# state_dict = torch.load(\"lstm_model_epoch_final.pt\")\n",
        "# print(state_dict.keys())\n",
        "# model.load_state_dict(state_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UbqdOpZPlQ7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "0a981ce8-d345-4f6a-f45e-5d65736cf075"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "window_size = 10\n",
        "plt.title(\"Validation Perplexity: Full Dataset\")\n",
        "plt.plot(model_ppls)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f032a351110>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMfab_b2yS3N"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na_Z6hljWxAn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "54741f0a-bb90-414f-faca-169de7e0cfea"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(MODEL_FOLDER+\"/\" + \"modelNewDecoder.pt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_2f9bdfd3-63a3-4867-a939-1f0d70f3d33f\", \"modelNewDecoder.pt\", 123556279)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIzaqGC4EiWQ"
      },
      "source": [
        "def greedy_decode(model, src_ids, src_lengths, max_len):\n",
        "  \"\"\"Greedily decode a sentence for EncoderDecoder. Make sure to chop off the \n",
        "     EOS token!\"\"\"\n",
        "  SOS_INDEX = vocab_oh[\"[start]\"]\n",
        "  EOS_INDEX = vocab_oh[\"[end]\"]\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    _, encoder_finals = model.encode(src_ids, src_lengths)\n",
        "    prev_y = torch.ones(1, 1).fill_(SOS_INDEX).type_as(src_ids)\n",
        "\n",
        "  output = []\n",
        "  hidden = None\n",
        "  \n",
        "  # --------- Your code here --------- #\n",
        "  for i in range(max_len):\n",
        "    with torch.no_grad():\n",
        "      hidden, pre_output = model.decode(encoder_finals, prev_y, hidden)\n",
        "      \n",
        "      prob = model.generator(pre_output[:,-1])\n",
        "    _, next_word = torch.max(prob,dim=1)\n",
        "    next_word = next_word.data.item()\n",
        "    \n",
        "    output.append(next_word)\n",
        "    prev_y = torch.ones(1, 1).fill_(next_word).type_as(src_ids)\n",
        "\n",
        "  output = np.array(output)\n",
        "  \n",
        "  if EOS_INDEX is not None:\n",
        "      first_eos = np.where(output==EOS_INDEX)[0]\n",
        "      if len(first_eos) > 0:\n",
        "          output = output[:first_eos[0]]   \n",
        "  # --------- Your code ends --------- #\n",
        "  \n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpX-FoL-SsiR",
        "outputId": "1a034aaa-8448-4713-9435-5cca336aba6b"
      },
      "source": [
        "\n",
        "import spacy\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "tokenizer = spacy.load('en_core_web_sm')\n",
        "\n",
        "LONGEST_MSG_LENGTH = len(longest_msg)\n",
        "for elt in train_data[:1]:\n",
        "  j = elt.index('[end]')\n",
        "  x = \" \".join(elt[1:j])\n",
        "  x = \"Goodbye\"\n",
        "  print(\"Human:\", x)\n",
        "  # x = input(\"Say something to the bot:\\n\")\n",
        "\n",
        "  x = x.lower()\n",
        "  PAD = ['[pad]']\n",
        "  tokenized_msg = tokenizer(x)\n",
        "  \n",
        "  \n",
        "  \n",
        "  parsed_msg = ' '.join([token.text for token in tokenized_msg])\n",
        "  split_msg = parsed_msg.split(' ')\n",
        "  \n",
        "  msg_len = min(len(split_msg) + 2, len(longest_msg))\n",
        "  if len(split_msg) < LONGEST_MSG_LENGTH:\n",
        "    padded_msg = START + split_msg + END + PAD*(LONGEST_MSG_LENGTH-len(split_msg)-2)\n",
        "    \n",
        "  else:\n",
        "    # raise ValueError(\"Message is too long!\")\n",
        "    padded_msg = START + split_msg[:len(longest_msg)-2] + END\n",
        "  \n",
        "  one_hotted_msg = torch.tensor(list(map(lambda word: vocab_oh[word], padded_msg))).unsqueeze(0)\n",
        "\n",
        "  y = greedy_decode(model, one_hotted_msg.to(device), torch.tensor(msg_len).unsqueeze(0).to(device),\n",
        "                          max_len=len(longest_msg))\n",
        "  output_msg = ' '.join([vocab[i] for i in y])\n",
        "  print(\"Bot:\",output_msg)\n",
        "  print(\"_________\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Human: Goodbye\n",
            "Bot: \n",
            "_________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlVRpRFYH2ON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd396bda-7a92-41e0-81ae-9a2d52e56d27"
      },
      "source": [
        "j\n",
        "train_data[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[start]',\n",
              " 'are',\n",
              " 'you',\n",
              " 'a',\n",
              " 'fan',\n",
              " 'of',\n",
              " 'google',\n",
              " 'or',\n",
              " 'microsoft',\n",
              " '?',\n",
              " '[end]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]',\n",
              " '[pad]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJRo83xhACan",
        "outputId": "2722cc6c-1163-4e14-a78a-20c76c460a96"
      },
      "source": [
        "print(y.shape)\n",
        "print(training_data[0][1].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6,)\n",
            "torch.Size([156])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCx2_kPQSs7g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "437dfc6e-4093-4a57-addc-dc19a9817e55"
      },
      "source": [
        "training_data[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([20629, 29499,  2981, 22659,  5159, 19913, 10126,  4235, 37572, 33702,\n",
              "         12993, 36425, 22075, 37576, 37212, 37212, 37212, 37212, 37212, 37212,\n",
              "         37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212,\n",
              "         37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212,\n",
              "         37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212,\n",
              "         37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212,\n",
              "         37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212,\n",
              "         37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212,\n",
              "         37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212,\n",
              "         37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212,\n",
              "         37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212,\n",
              "         37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212,\n",
              "         37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212,\n",
              "         37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212,\n",
              "         37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212,\n",
              "         37212, 37212, 37212, 37212, 37212, 37212]),\n",
              " tensor([20629, 13697,  4482, 22791,   494, 13697,  4761, 31565, 19966, 32307,\n",
              "         27326, 37310, 37576, 37212, 37212, 37212, 37212, 37212, 37212, 37212,\n",
              "         37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212,\n",
              "         37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212,\n",
              "         37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212,\n",
              "         37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212,\n",
              "         37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212,\n",
              "         37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212,\n",
              "         37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212,\n",
              "         37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212,\n",
              "         37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212,\n",
              "         37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212,\n",
              "         37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212,\n",
              "         37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212,\n",
              "         37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212, 37212,\n",
              "         37212, 37212, 37212, 37212, 37212, 37212]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0Gov_LD-rvk",
        "outputId": "a04f0572-aef9-4107-e05f-cb0350ff5028"
      },
      "source": [
        "l = []\n",
        "for elt in training_data[1][0]:\n",
        "  l.append(vocab[elt])\n",
        "print(l)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[start]', 'yeah', 'i', 'have', 'heard', 'that', 'before', 'as', 'well', '.', '', '', 'did', 'you', 'know', 'that', 'malaysian', 'lawmakers', 'recently', 'voted', 'to', 'abolish', 'the', 'mandatory', 'death', 'sentences', 'for', 'drug', 'offenses', '?', '[end]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]', '[pad]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHF3A_rz_HqF"
      },
      "source": [
        "test = \" \".join(l[1:31])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3VDvdXvt_OSY",
        "outputId": "4c19dae6-5b49-4736-c40f-fd95e2ee0cce"
      },
      "source": [
        "test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'yeah i have heard that before as well .   did you know that malaysian lawmakers recently voted to abolish the mandatory death sentences for drug offenses ? [end]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5L4JMsa_Onx",
        "outputId": "2c46fe35-823a-4612-b61c-c6b341c92047"
      },
      "source": [
        "test_tgt = \" \".join([vocab[elt] for elt in training_data[1][1]][1:11])\n",
        "print(test_tgt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "that sounds like positive progress ! i remember reading that\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ayO2A_J_awu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e49fef6-0f81-40fa-9f74-70fa61964cc4"
      },
      "source": [
        "len(train_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "157615"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74IsmqwrY7ql"
      },
      "source": [
        "6"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}